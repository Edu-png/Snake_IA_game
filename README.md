# üêç Snake Game com Aprendizado por Refor√ßo üí°

<p align="center">
  <a href="https://github.com/Edu-png">
    <img src="https://img.shields.io/badge/Autor-Eduardo%20Coqueiro-purple?style=flat&logo=github" alt="Autor">
  </a>
  <a href="mailto:eduardocoqueiro@gmail.com">
    <img src="https://img.shields.io/badge/Email-eduardocoqueiro%40gmail.com-purple?style=flat&logo=gmail" alt="Email">
  </a>
  <a href="https://linkedin.com/in/eduardocoqueiro/">
    <img src="https://img.shields.io/badge/LinkedIn-Eduardo%20Coqueiro-purple?style=flat&logo=linkedin" alt="LinkedIn">
  </a>
</p>

![CAPAS - PROJETOS (5)](https://github.com/user-attachments/assets/24de7a7c-c747-4bfb-a1b0-6f742e59e8ed)

## Sum√°rio üéØ

- [üêç Snake Game com Aprendizado por Refor√ßo üí°](#-snake-game-com-aprendizado-por-refor√ßo-)
  - [Resumo üìÑ](#resumo-)
  - [Objetivo üéØ](#objetivo-)
  - [Estrutura do Projeto üõ†](#estrutura-do-projeto-)
  - [Introdu√ß√£o ‚òÄ](#introdu√ß√£o-)
  - [Pipeline do Projeto üõ†](#pipeline-do-projeto-)
  - [Metodologia üß™](#metodologia-)
  - [Resultados e Conclus√µes üìà](#resultados-e-conclus√µes-)
    - [V√≠deo Inicial: Movimentos Aleat√≥rios üé•](#1-v√≠deo-inicial-movimentos-aleat√≥rios-)
    - [Progresso do Treinamento](#2-progresso-do-treinamento)
    - [V√≠deo Final: Agente Ap√≥s 1000+ Tentativas üé•](#3-v√≠deo-final-agente-ap√≥s-1000-tentativas-)
    - [Conclus√µes üöÄ](#conclus√µes-)
  - [Considera√ß√µes Finais üöÄ](#considera√ß√µes-finais-)
  - [Agradecimentos üëè](#agradecimentos-)
  - [üìû Contato](#-contato)


## Resumo üìÑ

Este projeto combina o cl√°ssico jogo da cobra com conceitos de aprendizado por refor√ßo, utilizando redes neurais e t√©cnicas de treinamento. A meta √© treinar um agente de IA capaz de maximizar a pontua√ß√£o no jogo de forma aut√¥noma. Atrav√©s do uso de bibliotecas como `Pygame`, `PyTorch` e `Numpy`, exploramos conceitos como:

- Redes neurais simples para tomada de decis√£o.
- Explora√ß√£o e explora√ß√£o durante o treinamento.
- T√©cnicas de otimiza√ß√£o como gradient descent e backpropagation.
- Visualiza√ß√£o de resultados com gr√°ficos de desempenho.

---

## Objetivo üéØ

O objetivo principal do projeto √©:

1. **Criar uma IA para o Snake Game:** Desenvolver um agente que aprenda a maximizar a pontua√ß√£o, utilizando aprendizado por refor√ßo.
2. **Explorar Algoritmos de Aprendizado:** Implementar uma rede neural que avalia estados do jogo e decide a√ß√µes com base em recompensas.
3. **Visualizar Progresso:** Monitorar o desempenho do agente durante o treinamento com gr√°ficos de pontua√ß√£o.
4. **Documentar o Processo:** Fornecer insights e etapas claras sobre a implementa√ß√£o, desafios e li√ß√µes aprendidas.

---

## Estrutura do Projeto üõ†

- **Jogabilidade Manual:** Baseada em `snake_game_human.py` para jogadores humanos.
- **Jogabilidade Automatizada com IA:** Utilizando `snake_game_ML.py` e a classe `Agent`.
- **Modelo e Treinamento:** Arquivo `model.py` com defini√ß√£o de rede neural e `agent.py` para l√≥gica de treinamento.
- **Visualiza√ß√£o:** Utilizando `helper.py` para gr√°ficos interativos de progresso.
- **Pipeline Completo:** Desde o treinamento at√© a execu√ß√£o do agente no jogo.

---
## Introdu√ß√£o ‚òÄ

Este projeto combina o cl√°ssico Snake Game com intelig√™ncia artificial, utilizando aprendizado por refor√ßo para treinar um agente que joga de forma aut√¥noma. A implementa√ß√£o envolve o uso de uma rede neural criada com `PyTorch` para avaliar estados do jogo e tomar decis√µes estrat√©gicas, enquanto o ambiente gr√°fico √© gerenciado pelo `Pygame`. Por meio de recompensas e penalidades, o agente aprende a maximizar sua pontua√ß√£o ao longo de v√°rias partidas, destacando conceitos como explora√ß√£o versus explora√ß√£o, ajuste de pesos na rede neural e otimiza√ß√£o baseada em gradientes. Este trabalho demonstra a aplica√ß√£o pr√°tica de algoritmos de IA em um problema cl√°ssico, proporcionando insights valiosos sobre aprendizado por refor√ßo.

## Pipeline do Projeto üõ†

A estrutura do projeto foi organizada para treinar uma IA capaz de jogar o Snake Game de forma aut√¥noma, com as seguintes etapas principais:

1. **Configura√ß√£o do Ambiente de Jogo:** Cria√ß√£o de um ambiente gr√°fico interativo utilizando `Pygame`.
2. **Defini√ß√£o da IA:** Implementa√ß√£o de uma rede neural simples com `PyTorch` para avaliar estados e tomar decis√µes.
3. **Treinamento do Agente:** Uso de aprendizado por refor√ßo para ajustar os pesos da rede neural, maximizando as recompensas.
4. **Monitoramento de Desempenho:** Visualiza√ß√£o da evolu√ß√£o da pontua√ß√£o atrav√©s de gr√°ficos interativos.
5. **Automa√ß√£o do Jogo:** Integra√ß√£o do agente treinado com o ambiente para jogar sem interven√ß√£o humana.

## Metodologia üß™

O desenvolvimento do projeto seguiu uma abordagem estruturada e iterativa, com as seguintes etapas detalhadas:

### 1. **Configura√ß√£o do Ambiente de Jogo**
- **Biblioteca Utilizada:** `Pygame` foi a base para criar o ambiente gr√°fico e simular o jogo Snake.
- **Desenvolvimento do Jogo Manual:** Um jogo inicial foi implementado (`snake_game_human.py`) para simular a jogabilidade manual, permitindo intera√ß√µes b√°sicas do jogador.
- **Componentes do Jogo:**
  - Movimento da cobra em quatro dire√ß√µes (cima, baixo, esquerda, direita).
  - Gera√ß√£o de alimentos em posi√ß√µes aleat√≥rias.
  - Detec√ß√£o de colis√µes com as bordas ou o pr√≥prio corpo.

---

### 2. **Defini√ß√£o da IA e Fun√ß√£o de Recompensa**
- **Modelo da Rede Neural:**
  - Arquivo `model.py` com a defini√ß√£o de uma rede neural simples (camadas lineares) implementada em `PyTorch`.
  - A arquitetura da rede possui tr√™s camadas: entrada (estado do jogo), oculta e sa√≠da (a√ß√µes poss√≠veis: virar √† esquerda, continuar reto ou virar √† direita).
- **Estados e A√ß√µes:**
  - Representa√ß√£o do estado do jogo em um vetor de 11 elementos que inclui:
    - Perigos pr√≥ximos (frente, esquerda, direita).
    - Dire√ß√£o atual da cobra.
    - Posi√ß√£o relativa do alimento.
  - A√ß√µes poss√≠veis:
    - Manter a dire√ß√£o atual.
    - Virar √† esquerda.
    - Virar √† direita.
- **Fun√ß√£o de Recompensa:**
  - +10 pontos por comer o alimento.
  - -10 pontos ao colidir com bordas ou com o pr√≥prio corpo.
  - Penalidades menores para movimentos ineficazes.

---

### 3. **Treinamento do Agente**
- **Treinamento de Mem√≥ria Curta e Longa:**
  - Implementa√ß√£o do agente no arquivo `agent.py` utilizando `deque` para armazenar mem√≥rias de jogadas anteriores.
  - Treinamento de mem√≥ria curta (ap√≥s cada jogada) e longa (em lotes maiores) para ajustar os pesos da rede neural.
- **Explora√ß√£o e Explora√ß√£o:**
  - Decis√£o probabil√≠stica entre explorar novas a√ß√µes ou explorar a√ß√µes conhecidas (definido pela taxa de explora√ß√£o `epsilon`).
- **Algoritmo de Otimiza√ß√£o:**
  - Uso de `Adam Optimizer` para minimizar o erro na previs√£o de recompensas futuras.
  - A fun√ß√£o de perda utilizada foi `MSELoss`.

---

### 4. **Automa√ß√£o do Jogo**
- **Execu√ß√£o da IA no Jogo:**
  - O agente treinado foi integrado ao ambiente (`snake_game_ML.py`) para jogar de forma aut√¥noma, tomando decis√µes com base nos estados do jogo e na pol√≠tica aprendida.
- **Limita√ß√£o de Itera√ß√µes:**
  - Para evitar loops infinitos, foi implementada uma limita√ß√£o no n√∫mero de frames sem progresso.

---

### 5. **Visualiza√ß√£o e Monitoramento**
- **Gr√°ficos de Progresso:**
  - O arquivo `helper.py` foi utilizado para gerar gr√°ficos interativos que mostram:
    - A pontua√ß√£o de cada jogo.
    - A pontua√ß√£o m√©dia ao longo do tempo.
  - Esses gr√°ficos foram atualizados em tempo real durante o treinamento.
- **M√©tricas de Desempenho:**
  - Compara√ß√£o da maior pontua√ß√£o alcan√ßada pelo agente em rela√ß√£o ao progresso geral.
  - Monitoramento da consist√™ncia do aprendizado atrav√©s da m√©dia de pontua√ß√£o.

---

### 6. **Aprimoramentos e Testes**
- **Valida√ß√£o do Modelo:**
  - Testes foram realizados com diferentes configura√ß√µes de hiperpar√¢metros (taxa de aprendizado, taxa de explora√ß√£o, e arquitetura da rede).
- **Itera√ß√µes de Treinamento:**
  - V√°rias partidas foram simuladas at√© que o agente mostrasse um desempenho consistente e otimizado.
- **Aprimoramento da Fun√ß√£o de Recompensa:**
  - Penalidades e recompensas foram ajustadas para incentivar o agente a adotar estrat√©gias mais eficazes.

---

### Ferramentas e Bibliotecas
- **Pygame:** Para criar o ambiente gr√°fico e simular o jogo.
- **PyTorch:** Para a implementa√ß√£o e treinamento da rede neural.
- **Numpy:** Para opera√ß√µes matem√°ticas e manipula√ß√£o de dados.
- **Matplotlib:** Para criar gr√°ficos interativos e visualizar o progresso.

Essa metodologia detalha como o projeto foi implementado, treinado e avaliado, cobrindo desde a configura√ß√£o inicial at√© as itera√ß√µes finais de aprimoramento.

## Resultados e Conclus√µes üìà

Os resultados do projeto demonstram a efic√°cia do aprendizado por refor√ßo na automa√ß√£o do Snake Game. O agente inicialmente apresentava movimentos aleat√≥rios e ineficientes, mas, com o passar do treinamento, come√ßou a tomar decis√µes mais inteligentes, aumentando sua pontua√ß√£o de forma consistente. 

### 1. V√≠deo Inicial: Movimentos Aleat√≥rios üé•
*Este v√≠deo mostra o comportamento da cobra antes do treinamento significativo, onde ela colide rapidamente devido √† aus√™ncia de estrat√©gias.*

[Espa√ßo reservado para o v√≠deo inicial]

---

### 2. Progresso do Treinamento
Os gr√°ficos e imagens a seguir ilustram o progresso do agente em diferentes est√°gios de treinamento:

#### Ap√≥s 50 Tentativas
[Espa√ßo reservado para imagem da cobra ap√≥s 50 tentativas]

#### Ap√≥s 100 Tentativas
[Espa√ßo reservado para imagem da cobra ap√≥s 100 tentativas]

#### Ap√≥s 300 Tentativas
[Espa√ßo reservado para imagem da cobra ap√≥s 300 tentativas]

#### Ap√≥s 700 Tentativas
[Espa√ßo reservado para imagem da cobra ap√≥s 700 tentativas]

#### Ap√≥s 1000 Tentativas
[Espa√ßo reservado para imagem da cobra ap√≥s 1000 tentativas]

---

### 3. V√≠deo Final: Agente Ap√≥s 1000+ Tentativas üé•
*Este v√≠deo mostra o agente jogando de forma otimizada, com decis√µes r√°pidas e estrat√©gias eficazes para evitar colis√µes e maximizar a pontua√ß√£o.*

[Espa√ßo reservado para o v√≠deo final]

---

### Conclus√µes üöÄ

1. **Aprendizado Progressivo:** 
   - O agente apresentou uma curva de aprendizado clara, passando de movimentos aleat√≥rios para estrat√©gias otimizadas ap√≥s 1000 tentativas.

2. **Impacto da Fun√ß√£o de Recompensa:** 
   - Ajustes na fun√ß√£o de recompensa foram cruciais para incentivar o agente a adotar comportamentos mais eficazes, como evitar colis√µes e buscar alimentos.

3. **Visualiza√ß√£o do Progresso:**
   - Os gr√°ficos demonstraram uma melhoria consistente na pontua√ß√£o m√©dia ao longo do tempo, indicando um treinamento eficaz.

4. **Pr√≥ximos Passos:**
   - Explorar arquiteturas de rede mais complexas.
   - Implementar uma interface gr√°fica mais interativa e detalhada.
   - Adicionar m√©tricas para medir a efici√™ncia energ√©tica do agente (n√∫mero de movimentos por alimento).

Com essas conclus√µes, o projeto demonstra como conceitos te√≥ricos de aprendizado por refor√ßo podem ser aplicados a problemas pr√°ticos, gerando resultados significativos e proporcionando insights sobre intelig√™ncia artificial.

## Considera√ß√µes Finais üöÄ

Este projeto demonstrou a aplica√ß√£o pr√°tica do aprendizado por refor√ßo em um ambiente simulado, como o Snake Game, explorando conceitos fundamentais de intelig√™ncia artificial e otimiza√ß√£o. Atrav√©s do desenvolvimento e treinamento do agente, foram alcan√ßados resultados significativos, destacando a evolu√ß√£o progressiva do comportamento da IA ao longo de v√°rias tentativas.

### Pontos de Destaque:
1. **Evolu√ß√£o do Agente:** 
   - O agente evoluiu de movimentos aleat√≥rios para estrat√©gias inteligentes, demonstrando a capacidade do aprendizado por refor√ßo em adaptar-se a desafios e melhorar seu desempenho.
2. **Flexibilidade e Escalabilidade:** 
   - A implementa√ß√£o, baseada em bibliotecas robustas como `PyTorch` e `Pygame`, oferece uma base s√≥lida para expandir o projeto com arquiteturas de rede mais avan√ßadas ou novos ambientes de teste.
3. **Visualiza√ß√£o e Monitoramento:** 
   - A utiliza√ß√£o de gr√°ficos para monitorar o progresso do treinamento foi fundamental para entender os avan√ßos e ajustar os par√¢metros do modelo.

### Limita√ß√µes e Melhorias Futuras:
1. **Simplicidade do Modelo:** 
   - A rede neural utilizada √© relativamente simples, podendo ser substitu√≠da por modelos mais sofisticados para explorar padr√µes complexos no ambiente.
2. **Explora√ß√£o de Hiperpar√¢metros:** 
   - Estudos mais aprofundados sobre hiperpar√¢metros, como taxa de aprendizado e gama de desconto, podem levar a um treinamento mais eficiente.
3. **Ambiente Simples:** 
   - O Snake Game apresenta um ambiente est√°tico e previs√≠vel. Projetos futuros podem incorporar ambientes din√¢micos ou multijogadores para aumentar a complexidade do aprendizado.

### Impacto e Aprendizados:
Este projeto evidenciou como um problema cl√°ssico de jogo pode ser transformado em um desafio de aprendizado por refor√ßo, proporcionando n√£o apenas resultados pr√°ticos, mas tamb√©m um aprendizado valioso sobre conceitos como explora√ß√£o, explora√ß√£o e ajustes de recompensas.

Com a possibilidade de melhorias cont√≠nuas e aplica√ß√µes em problemas mais complexos, este projeto se apresenta como uma base s√≥lida para explorar os horizontes da intelig√™ncia artificial em ambientes simulados e no mundo real.

## Agradecimentos üëè

Gostaria de expressar minha gratid√£o a todas as pessoas e recursos que contribu√≠ram para a realiza√ß√£o deste projeto. Em especial:

1. **Comunidade de Desenvolvedores:** Pelas bibliotecas poderosas como `PyTorch`, `Pygame` e `Matplotlib`, que tornaram este projeto poss√≠vel.
2. **Criadores de Conte√∫do:** O v√≠deo original que inspirou este projeto serviu como uma fonte rica de conhecimento e direcionamento pr√°tico para implementar aprendizado por refor√ßo no Snake Game.
3. **Plataformas de Aprendizado:** Cursos e materiais online foram essenciais para aprofundar os conhecimentos necess√°rios na implementa√ß√£o de redes neurais e algoritmos de aprendizado por refor√ßo.

Este projeto n√£o seria poss√≠vel sem essas contribui√ß√µes diretas e indiretas. A todos, meu muito obrigado!

<div align="center">
  <img src="https://github.com/user-attachments/assets/54afb33c-97be-40b6-8c96-0f12852e946f" alt="thank-you" width="500">
</div>

## üìû Contato
- **LinkedIn:** [Eduardo Coqueiro](https://www.linkedin.com/in/eduardocoqueiro/)
- **Site:** [Eduardo Coqueiro](https://dataguy.my.canva.site/eduardo-coqueiro)
- **Kaggle:** [Eduardo Coqueiro](https://www.kaggle.com/eduardocoqueiro)

